{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gestures = [\"swipe_up\", \"swipe_down\", \"swipe_left\", \"swipe_right\", \n",
    "            \"pointing\", \"open_palm\", \"thumb_index_touch\", \"fist\"]\n",
    "\n",
    "dynamic = [\"swipe_up\", \"swipe_down\", \"swipe_left\", \"swipe_right\"]\n",
    "static = [\"pointing\", \"open_palm\", \"thumb_index_touch\", \"fist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(landmarks):\n",
    "    landmarks = np.array(landmarks).reshape(-1, 3)\n",
    "    wrist = landmarks[0]\n",
    "    landmarks = landmarks - wrist\n",
    "    max_dist = np.max(np.linalg.norm(landmarks, axis=1))\n",
    "    if max_dist > 0:\n",
    "        landmarks = landmarks / max_dist\n",
    "    landmarks = landmarks.flatten()\n",
    "    landmarks = torch.FloatTensor(landmarks)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticGestureModel(nn.Module):\n",
    "    def __init__(self, input_size=63, num_classes=5):\n",
    "        super(StaticGestureModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "     \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dir):\n",
    "    train_dir = {}\n",
    "    val_dir = {}\n",
    "    test_dir = {}\n",
    "    for cls in static:\n",
    "        cls_dir = os.path.join(dir, cls)\n",
    "        if not os.path.isdir(cls_dir): continue\n",
    "        json_files = [os.path.join(cls_dir, f) for f in os.listdir(cls_dir) if f.endswith('.json')]\n",
    "        random.shuffle(json_files)\n",
    "        n = len(json_files)\n",
    "        split_1 = int(0.8 * n)\n",
    "        split_2 = split_1 + int(0.1 * n)\n",
    "\n",
    "        train_dir[cls] = json_files[:split_1]\n",
    "        val_dir[cls] = json_files[split_1:split_2]\n",
    "        test_dir[cls] = json_files[split_2:]\n",
    "    \n",
    "    return train_dir, val_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticGestureDataset(Dataset):\n",
    "    def __init__(self, data_dir:dict, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory with gesture data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.classes = list(data_dir.keys())\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            for jf_path in data_dir[cls]:\n",
    "                with open(jf_path, 'r') as f:\n",
    "                    landmarks = json.load(f)\n",
    "                    landmarks = preprocess_landmarks(landmarks).numpy()\n",
    "                    self.data.append(landmarks)\n",
    "                    self.labels.append(self.class_to_idx[cls])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        else:\n",
    "            sample = torch.FloatTensor(sample)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_static_gesture_model():\n",
    "    # Paths\n",
    "    static_dir = os.getcwd() + '/gesture_dataset/static'\n",
    "    # Hyperparameters\n",
    "    num_epochs = 50\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "    num_classes = len(static)\n",
    "    train_data, val_data, test_data = split_data(static_dir)\n",
    "    # Datasets and Dataloaders\n",
    "    train_dataset = StaticGestureDataset(train_data)\n",
    "    val_dataset = StaticGestureDataset(val_data)\n",
    "    test_dataset = StaticGestureDataset(test_data)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Model, Loss, Optimizer\n",
    "    model = StaticGestureModel(input_size=63, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(val_dataset)\n",
    "        val_epoch_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "              f'Train Loss: {epoch_loss:.4f} Train Acc: {epoch_acc:.2f}% '\n",
    "              f'Val Loss: {val_epoch_loss:.4f} Val Acc: {val_epoch_acc:.2f}%')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save(model.state_dict(), 'models/static_gesture_model.pth')\n",
    "            print(f'Best model saved with Val Acc: {best_val_acc:.2f}%')\n",
    "    \n",
    "    model.load_state_dict(torch.load('models/static_gesture_model.pth'))\n",
    "    model.to(device)\n",
    "    train_acc = evaluate(model, train_loader, device)\n",
    "    \n",
    "    # Evaluate on Validation Set\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Evaluate on Test Set\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    print(\"\\nFinal Accuracies of the Best Model:\")\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Epoch [1/1000] Train Loss: 2.3002 Train Acc: 20.34% Val Loss: 2.1032 Val Acc: 37.27%\n",
      "Best model saved with Val Acc: 37.27%\n",
      "Epoch [2/1000] Train Loss: 2.0193 Train Acc: 34.94% Val Loss: 1.7445 Val Acc: 52.73%\n",
      "Best model saved with Val Acc: 52.73%\n",
      "Epoch [3/1000] Train Loss: 1.6984 Train Acc: 47.22% Val Loss: 1.3884 Val Acc: 66.82%\n",
      "Best model saved with Val Acc: 66.82%\n",
      "Epoch [4/1000] Train Loss: 1.3937 Train Acc: 55.68% Val Loss: 1.1106 Val Acc: 70.45%\n",
      "Best model saved with Val Acc: 70.45%\n",
      "Epoch [5/1000] Train Loss: 1.1820 Train Acc: 63.01% Val Loss: 0.9256 Val Acc: 75.00%\n",
      "Best model saved with Val Acc: 75.00%\n",
      "Epoch [6/1000] Train Loss: 1.0399 Train Acc: 65.91% Val Loss: 0.7793 Val Acc: 77.73%\n",
      "Best model saved with Val Acc: 77.73%\n",
      "Epoch [7/1000] Train Loss: 0.9439 Train Acc: 69.15% Val Loss: 0.7091 Val Acc: 75.45%\n",
      "Epoch 8/1000:   0%|           | 0/28 [00:00<?, ?it/s, accuracy=67.2, loss=0.791]^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/RealGestureX/scripts/train_static.py\", line 180, in <module>\n",
      "    train_static_gesture_model()\n",
      "  File \"/home/jupyter/RealGestureX/scripts/train_static.py\", line 100, in train_static_gesture_model\n",
      "    inputs, labels = inputs.to(device), labels.to(device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 -m scripts.train_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
